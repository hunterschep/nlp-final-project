## Literature Review

### Verónica Pérez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea. 2018. *Automatic Detection of Fake News*.  
*In Proceedings of the 27th International Conference on Computational Linguistics, pages 3391–3401, Santa Fe, New Mexico, USA. Association for Computational Linguistics.*

- **Data**: Crowd-sourced fake news data (mechanical turk, 240 total), news from “ABCNews, CNN, USAToday, NewYorkTimes, FoxNews, Bloomberg, and CNET among others” (240 total, manually fact-checked), fake celebrity news (magazines, 250 total), real celebrity news (magazines, 250 total)  
- **Experiments**: used 5 features (n-grams, punctuation, Psycholinguistic features, readability, syntax), SVM, “the machine learning algorithms implementation available in the caret (Kuhn et al., 2016) and e1071 packages”, trained on different combinations of features (feature selection)  
- **Evaluation**: five-fold cross-validation, with accuracy, precision, recall, and F-score  
- **Results**: Random Baseline of 50%, “our results suggest that fake news differ from real news mainly in aspects such as writing style (punctuation, readability, syntactic structure) and aspects related to writer’s internal processes (LIWC features).”, “using all the features on the two datasets we achieve the best accuracies, with 0.74 and 0.76 respectively.” (fake news, then celebrity)  
- **Relevance to our work**: similar experimental approach, gives confidence to our approach using classing ML models and the features we plan on using (could inform some that we choose), key part of our project is determining news, this study did that.  

---

We find work that is relevant to our project, specifically our goals and methods, in the work of Pérez-Rosas, Kleinberg, et al. (2018). Their work focused on determining fake news sources from real news sources, through similar models and approaches to what we have discussed. Their data comes from multiple sources, 240 fake-news examples generated by humans via Mechanical Turk, 240 independently verified real news articles taken from about 10 credible news sources (CNN, NYT, etc), 250 fake news stories about celebrities verified as false taken from tabloids, and 250 real news about celebrities stories verified as true taken from tabloids. They then extracted 5 distinct features from the data (n-grams, punctuation, Psycholinguistic features, readability, syntax). This is relevant to our project as we plan to use a similar approach focusing on n-grams, but have considered using some of these other features. Pérez-Rosas, Kleinberg, et al. (2018) continued to experiment with SVM and other basic machine learning models trained on different combinations of their features. This method is also relevant to our project, as we plan on experimenting with similar machine learning models in our process. They evaluated with a standard five-fold cross-validation, with accuracy, precision, recall, and F-score. Their results were a significant improvement over their 50% random baseline, achieving accuracies up to 74% on the fake news dataset and 76% on the celebrity news dataset. As mentioned, multiple aspects of this study are relevant to our project, specifically the data, features, and methods. A key part of our project is determining the validity of a news segment by attributing it to a source, and the success of their experiment gives us confidence in our own.

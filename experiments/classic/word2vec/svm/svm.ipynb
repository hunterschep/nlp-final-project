{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3c58cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "     --------------------------------------- 24.0/24.0 MB 54.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\gpete\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.8.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\gpete\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.23.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.7/61.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: wrapt in c:\\users\\gpete\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.3 smart-open-7.1.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpete\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, Phrases, phrases\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing, warnings, re, string, os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e528e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          publication                                      clean_article  \\\n",
      "0  The New York Times   a love of [NAME] and slap bracelets, [NAME] s...   \n",
      "1  The New York Times  warm, occasionally downright balmy, weather, a...   \n",
      "2  The New York Times  dably confused. When he was a boy, Havana was ...   \n",
      "\n",
      "   split  \n",
      "0  train  \n",
      "1  train  \n",
      "2  train  \n"
     ]
    }
   ],
   "source": [
    "# read in the data and only select the article and the publication\n",
    "# and getting them split into training and testing\n",
    "DATA_PATH = \"../../../../data/all-the-news-2-1-SMALL-CLEANED.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.head(3)[[\"publication\", \"clean_article\", \"split\"]])\n",
    "\n",
    "# Split provided by the file\n",
    "df_train = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "df_test  = df[df[\"split\"] == \"test\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29819f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:17<00:00, 5037.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenizer(text: str):\n",
    "    text = text.lower()\n",
    "    # remove punctuation but keep intra‑word ’ characters if any\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    return wordpunct_tokenize(text)\n",
    "\n",
    "# Tokenize every article (lazy eval with progress bar)\n",
    "train_tokens = df_train[\"clean_article\"].progress_map(simple_tokenizer)\n",
    "test_tokens  = df_test[\"clean_article\"].map(simple_tokenizer)\n",
    "\n",
    "# Learn bigram phrases from training corpus\n",
    "bigram_phrases = Phrases(train_tokens, min_count=5, threshold=10)\n",
    "bigram_phraser = phrases.Phraser(bigram_phrases)\n",
    "\n",
    "# Apply bigrams\n",
    "train_tokens = train_tokens.apply(lambda x: bigram_phraser[x])\n",
    "test_tokens  = test_tokens.apply(lambda x: bigram_phraser[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc62238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec epochs: 100%|██████████| 10/10 [05:39<00:00, 34.00s/it, loss=69519680.00]\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM  = 200        # vector size\n",
    "WINDOW     = 5\n",
    "MIN_COUNT  = 3\n",
    "SG         = 1          # 1 = skip‑gram, 0 = CBOW\n",
    "EPOCHS     = 10\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Logs loss & shows a tqdm bar for each epoch.\"\"\"\n",
    "    def __init__(self, total_epochs):\n",
    "        self.epoch     = 0\n",
    "        self.pbar      = tqdm(total=total_epochs, desc=\"Word2Vec epochs\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        self.pbar.set_postfix({\"loss\": f\"{loss:.2f}\"})\n",
    "        self.pbar.update(1)\n",
    "        self.epoch += 1\n",
    "        if self.epoch == self.pbar.total:\n",
    "            self.pbar.close()\n",
    "\n",
    "logger = EpochLogger(EPOCHS)\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=list(train_tokens),\n",
    "    vector_size=EMBED_DIM,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=multiprocessing.cpu_count() - 1,\n",
    "    sg=SG,\n",
    "    epochs=EPOCHS,\n",
    "    compute_loss=True,      # required to query loss\n",
    "    callbacks=[logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791adce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [01:02<00:00, 1433.18it/s]\n",
      "100%|██████████| 10000/10000 [00:06<00:00, 1432.76it/s]\n"
     ]
    }
   ],
   "source": [
    "def sent_vector(tokens, model, dim):\n",
    "    \"\"\"Average the word vectors for tokens present in the model’s vocab.\n",
    "       Returns a zero‑vector if no token is in the vocab.\"\"\"\n",
    "    valid_vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not valid_vecs:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(valid_vecs, axis=0)\n",
    "\n",
    "# Vectorize train & test articles\n",
    "X_train = np.vstack([sent_vector(tok, w2v_model, EMBED_DIM) for tok in tqdm(train_tokens)])\n",
    "X_test  = np.vstack([sent_vector(tok, w2v_model, EMBED_DIM) for tok in tqdm(test_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10327a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "y_train = lbl.fit_transform(df_train[\"publication\"])\n",
    "y_test  = lbl.transform(df_test[\"publication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c63fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "Accuracy: 0.7751\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Buzzfeed News       0.63      0.50      0.56      1000\n",
      "               CNN       0.67      0.72      0.69      1000\n",
      "         Economist       0.88      0.95      0.91      1000\n",
      "          Fox News       0.76      0.73      0.74      1000\n",
      "            People       0.81      0.91      0.86      1000\n",
      "          Politico       0.65      0.70      0.68      1000\n",
      "           Reuters       0.90      0.91      0.90      1000\n",
      "          The Hill       0.87      0.78      0.82      1000\n",
      "The New York Times       0.79      0.72      0.76      1000\n",
      "              Vice       0.78      0.81      0.80      1000\n",
      "\n",
      "          accuracy                           0.78     10000\n",
      "         macro avg       0.77      0.78      0.77     10000\n",
      "      weighted avg       0.77      0.78      0.77     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "clf = LinearSVC(               \n",
    "    C=1.0,                         \n",
    "    max_iter=1000,                  \n",
    "    multi_class=\"ovr\", \n",
    "    verbose=2                      \n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=lbl.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5502e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ea198_row0_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 95.0%, transparent 95.0%);\n",
       "}\n",
       "#T_ea198_row1_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 91.4%, transparent 91.4%);\n",
       "}\n",
       "#T_ea198_row2_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 91.1%, transparent 91.1%);\n",
       "}\n",
       "#T_ea198_row3_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 81.3%, transparent 81.3%);\n",
       "}\n",
       "#T_ea198_row4_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 78.4%, transparent 78.4%);\n",
       "}\n",
       "#T_ea198_row5_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 72.8%, transparent 72.8%);\n",
       "}\n",
       "#T_ea198_row6_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 72.5%, transparent 72.5%);\n",
       "}\n",
       "#T_ea198_row7_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 72.3%, transparent 72.3%);\n",
       "}\n",
       "#T_ea198_row8_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 70.5%, transparent 70.5%);\n",
       "}\n",
       "#T_ea198_row9_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 49.8%, transparent 49.8%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ea198\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ea198_level0_col0\" class=\"col_heading level0 col0\" >publication</th>\n",
       "      <th id=\"T_ea198_level0_col1\" class=\"col_heading level0 col1\" >n_test</th>\n",
       "      <th id=\"T_ea198_level0_col2\" class=\"col_heading level0 col2\" >correct</th>\n",
       "      <th id=\"T_ea198_level0_col3\" class=\"col_heading level0 col3\" >accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "      <td id=\"T_ea198_row0_col0\" class=\"data row0 col0\" >Economist</td>\n",
       "      <td id=\"T_ea198_row0_col1\" class=\"data row0 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row0_col2\" class=\"data row0 col2\" >950</td>\n",
       "      <td id=\"T_ea198_row0_col3\" class=\"data row0 col3\" >0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
       "      <td id=\"T_ea198_row1_col0\" class=\"data row1 col0\" >People</td>\n",
       "      <td id=\"T_ea198_row1_col1\" class=\"data row1 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row1_col2\" class=\"data row1 col2\" >914</td>\n",
       "      <td id=\"T_ea198_row1_col3\" class=\"data row1 col3\" >0.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row2\" class=\"row_heading level0 row2\" >6</th>\n",
       "      <td id=\"T_ea198_row2_col0\" class=\"data row2 col0\" >Reuters</td>\n",
       "      <td id=\"T_ea198_row2_col1\" class=\"data row2 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row2_col2\" class=\"data row2 col2\" >911</td>\n",
       "      <td id=\"T_ea198_row2_col3\" class=\"data row2 col3\" >0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row3\" class=\"row_heading level0 row3\" >9</th>\n",
       "      <td id=\"T_ea198_row3_col0\" class=\"data row3 col0\" >Vice</td>\n",
       "      <td id=\"T_ea198_row3_col1\" class=\"data row3 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row3_col2\" class=\"data row3 col2\" >813</td>\n",
       "      <td id=\"T_ea198_row3_col3\" class=\"data row3 col3\" >0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row4\" class=\"row_heading level0 row4\" >7</th>\n",
       "      <td id=\"T_ea198_row4_col0\" class=\"data row4 col0\" >The Hill</td>\n",
       "      <td id=\"T_ea198_row4_col1\" class=\"data row4 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row4_col2\" class=\"data row4 col2\" >784</td>\n",
       "      <td id=\"T_ea198_row4_col3\" class=\"data row4 col3\" >0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row5\" class=\"row_heading level0 row5\" >3</th>\n",
       "      <td id=\"T_ea198_row5_col0\" class=\"data row5 col0\" >Fox News</td>\n",
       "      <td id=\"T_ea198_row5_col1\" class=\"data row5 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row5_col2\" class=\"data row5 col2\" >728</td>\n",
       "      <td id=\"T_ea198_row5_col3\" class=\"data row5 col3\" >0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row6\" class=\"row_heading level0 row6\" >1</th>\n",
       "      <td id=\"T_ea198_row6_col0\" class=\"data row6 col0\" >CNN</td>\n",
       "      <td id=\"T_ea198_row6_col1\" class=\"data row6 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row6_col2\" class=\"data row6 col2\" >725</td>\n",
       "      <td id=\"T_ea198_row6_col3\" class=\"data row6 col3\" >0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row7\" class=\"row_heading level0 row7\" >8</th>\n",
       "      <td id=\"T_ea198_row7_col0\" class=\"data row7 col0\" >The New York Times</td>\n",
       "      <td id=\"T_ea198_row7_col1\" class=\"data row7 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row7_col2\" class=\"data row7 col2\" >723</td>\n",
       "      <td id=\"T_ea198_row7_col3\" class=\"data row7 col3\" >0.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row8\" class=\"row_heading level0 row8\" >5</th>\n",
       "      <td id=\"T_ea198_row8_col0\" class=\"data row8 col0\" >Politico</td>\n",
       "      <td id=\"T_ea198_row8_col1\" class=\"data row8 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row8_col2\" class=\"data row8 col2\" >705</td>\n",
       "      <td id=\"T_ea198_row8_col3\" class=\"data row8 col3\" >0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea198_level0_row9\" class=\"row_heading level0 row9\" >0</th>\n",
       "      <td id=\"T_ea198_row9_col0\" class=\"data row9 col0\" >Buzzfeed News</td>\n",
       "      <td id=\"T_ea198_row9_col1\" class=\"data row9 col1\" >1000</td>\n",
       "      <td id=\"T_ea198_row9_col2\" class=\"data row9 col2\" >498</td>\n",
       "      <td id=\"T_ea198_row9_col3\" class=\"data row9 col3\" >0.498000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x201b93c58a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ❶ Confusion‑matrix rows = true labels, cols = predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred, labels=range(len(lbl.classes_)))\n",
    "\n",
    "# ❷ Diagonal elements are “hits” for each class\n",
    "hits          = cm.diagonal()\n",
    "total_true    = cm.sum(axis=1)\n",
    "acc_per_class = hits / total_true\n",
    "\n",
    "acc_df = pd.DataFrame({\n",
    "    \"publication\": lbl.classes_,\n",
    "    \"n_test\":      total_true,\n",
    "    \"correct\":     hits,\n",
    "    \"accuracy\":    acc_per_class.round(3)\n",
    "}).sort_values(\"accuracy\", ascending=False)\n",
    "\n",
    "display(acc_df.style.bar(subset=[\"accuracy\"], vmin=0, vmax=1, color='#66c2a5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec -> Logistic Regression\n",
    "\n",
    "Using Word2Vec embeddings with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas scikit-learn gensim nltk tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hunterschep/.pyenv/versions/3.12.5/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, Phrases, phrases\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing, warnings, re, string, os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          publication                                      clean_article  \\\n",
      "0  The New York Times   a love of [NAME] and slap bracelets, [NAME] s...   \n",
      "1  The New York Times  warm, occasionally downright balmy, weather, a...   \n",
      "2  The New York Times  dably confused. When he was a boy, Havana was ...   \n",
      "\n",
      "   split  \n",
      "0  train  \n",
      "1  train  \n",
      "2  train  \n",
      "Train rows: 90,000  •  Test rows: 10,000\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../../../../data/all-the-news-2-1-SMALL-CLEANED.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.head(3)[[\"publication\", \"clean_article\", \"split\"]])\n",
    "\n",
    "# Split provided by the file\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train rows: {len(train_df):,}  •  Test rows: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:10<00:00, 8405.58it/s] \n"
     ]
    }
   ],
   "source": [
    "def simple_tokenizer(text: str):\n",
    "    text = text.lower()\n",
    "    # remove punctuation but keep intra‑word ’ characters if any\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    return wordpunct_tokenize(text)\n",
    "\n",
    "# Tokenize every article (lazy eval with progress bar)\n",
    "train_tokens = train_df[\"clean_article\"].progress_map(simple_tokenizer)\n",
    "test_tokens  = test_df[\"clean_article\"].map(simple_tokenizer)\n",
    "\n",
    "# Learn bigram phrases from training corpus e.g \"united states\" -> \"united_states\"\n",
    "bigram_phrases = Phrases(train_tokens, min_count=5, threshold=10)\n",
    "bigram_phraser = phrases.Phraser(bigram_phrases)\n",
    "\n",
    "# merge common tokens into 1 token\n",
    "train_tokens = train_tokens.apply(lambda x: bigram_phraser[x])\n",
    "test_tokens  = test_tokens.apply(lambda x: bigram_phraser[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Word2Vec epochs: 100%|██████████| 10/10 [04:01<00:00, 24.15s/it, loss=70295008.00]\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM  = 200        # vector size\n",
    "WINDOW     = 5\n",
    "MIN_COUNT  = 3\n",
    "SG         = 1          # 1 = skip‑gram, 0 = CBOW\n",
    "EPOCHS     = 10\n",
    "\n",
    "# Add some logging to the Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Logs loss & shows a tqdm bar for each epoch.\"\"\"\n",
    "    def __init__(self, total_epochs):\n",
    "        self.epoch     = 0\n",
    "        self.pbar      = tqdm(total=total_epochs, desc=\"Word2Vec epochs\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        self.pbar.set_postfix({\"loss\": f\"{loss:.2f}\"})\n",
    "        self.pbar.update(1)\n",
    "        self.epoch += 1\n",
    "        if self.epoch == self.pbar.total:\n",
    "            self.pbar.close()\n",
    "\n",
    "logger = EpochLogger(EPOCHS)\n",
    "\n",
    "# Initialize the Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=list(train_tokens),\n",
    "    vector_size=EMBED_DIM,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=multiprocessing.cpu_count() - 1,\n",
    "    sg=SG,\n",
    "    epochs=EPOCHS,\n",
    "    compute_loss=True,      # required to query loss\n",
    "    callbacks=[logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [00:31<00:00, 2880.61it/s]\n",
      "100%|██████████| 10000/10000 [00:03<00:00, 2723.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to a vector\n",
    "def sent_vector(tokens, model, dim):\n",
    "    \"\"\"Average the word vectors for tokens present in the model’s vocab.\n",
    "       Returns a zero‑vector if no token is in the vocab.\"\"\"\n",
    "    valid_vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not valid_vecs:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(valid_vecs, axis=0)\n",
    "\n",
    "# Vectorize train & test articles\n",
    "X_train = np.vstack([sent_vector(tok, w2v_model, EMBED_DIM) for tok in tqdm(train_tokens)])\n",
    "X_test  = np.vstack([sent_vector(tok, w2v_model, EMBED_DIM) for tok in tqdm(test_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "y_train = lbl.fit_transform(train_df[\"publication\"])\n",
    "y_test  = lbl.transform(test_df[\"publication\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 14 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➜ fitting multinomial logistic regression …\n",
      "Epoch 1, change: 1\n",
      "Epoch 2, change: 0.16569284\n",
      "Epoch 3, change: 0.07678894\n",
      "Epoch 4, change: 0.054561283\n",
      "Epoch 5, change: 0.035400048\n",
      "Epoch 6, change: 0.025132937\n",
      "Epoch 7, change: 0.019676723\n",
      "Epoch 8, change: 0.014282386\n",
      "Epoch 9, change: 0.01162727\n",
      "Epoch 10, change: 0.0092766006\n",
      "Epoch 11, change: 0.00705009\n",
      "Epoch 12, change: 0.0064125452\n",
      "Epoch 13, change: 0.0059442748\n",
      "Epoch 14, change: 0.0056426586\n",
      "Epoch 15, change: 0.0052593956\n",
      "Epoch 16, change: 0.0048964145\n",
      "Epoch 17, change: 0.0044792225\n",
      "Epoch 18, change: 0.0042592487\n",
      "Epoch 19, change: 0.0040646056\n",
      "Epoch 20, change: 0.0037165384\n",
      "Epoch 21, change: 0.0036619639\n",
      "Epoch 22, change: 0.003476426\n",
      "Epoch 23, change: 0.0032403467\n",
      "Epoch 24, change: 0.0030288748\n",
      "Epoch 25, change: 0.0027148959\n",
      "Epoch 26, change: 0.0025430794\n",
      "Epoch 27, change: 0.0024351915\n",
      "Epoch 28, change: 0.0023053747\n",
      "Epoch 29, change: 0.0022139149\n",
      "Epoch 30, change: 0.0020882462\n",
      "Epoch 31, change: 0.001911258\n",
      "Epoch 32, change: 0.0018391768\n",
      "Epoch 33, change: 0.001725319\n",
      "Epoch 34, change: 0.0016975233\n",
      "Epoch 35, change: 0.0016028053\n",
      "Epoch 36, change: 0.0014978923\n",
      "Epoch 37, change: 0.0014339754\n",
      "Epoch 38, change: 0.0013365417\n",
      "Epoch 39, change: 0.0012327662\n",
      "Epoch 40, change: 0.0011356564\n",
      "Epoch 41, change: 0.00094290072\n",
      "Epoch 42, change: 0.00086153054\n",
      "Epoch 43, change: 0.0007747577\n",
      "Epoch 44, change: 0.00071256451\n",
      "Epoch 45, change: 0.00063441799\n",
      "Epoch 46, change: 0.00068642671\n",
      "Epoch 47, change: 0.00059920101\n",
      "Epoch 48, change: 0.00052351941\n",
      "Epoch 49, change: 0.00051160174\n",
      "Epoch 50, change: 0.000469747\n",
      "Epoch 51, change: 0.00041538684\n",
      "Epoch 52, change: 0.00045720566\n",
      "Epoch 53, change: 0.0004368815\n",
      "Epoch 54, change: 0.00039552632\n",
      "Epoch 55, change: 0.00034750634\n",
      "Epoch 56, change: 0.00036205322\n",
      "Epoch 57, change: 0.00030591004\n",
      "Epoch 58, change: 0.00029424761\n",
      "Epoch 59, change: 0.00029502029\n",
      "Epoch 60, change: 0.00028644828\n",
      "Epoch 61, change: 0.00028414509\n",
      "Epoch 62, change: 0.00025213443\n",
      "Epoch 63, change: 0.00025820613\n",
      "Epoch 64, change: 0.00023428469\n",
      "Epoch 65, change: 0.00021979584\n",
      "Epoch 66, change: 0.00023195241\n",
      "Epoch 67, change: 0.00020043759\n",
      "Epoch 68, change: 0.00024166438\n",
      "Epoch 69, change: 0.00017753842\n",
      "Epoch 70, change: 0.00019576354\n",
      "Epoch 71, change: 0.0002476142\n",
      "Epoch 72, change: 0.00018734425\n",
      "Epoch 73, change: 0.00016908161\n",
      "Epoch 74, change: 0.00022999987\n",
      "Epoch 75, change: 0.00015416801\n",
      "Epoch 76, change: 0.00022822483\n",
      "Epoch 77, change: 0.00023513928\n",
      "Epoch 78, change: 0.00017365202\n",
      "Epoch 79, change: 0.00015832385\n",
      "Epoch 80, change: 0.00019873457\n",
      "Epoch 81, change: 0.00018299067\n",
      "Epoch 82, change: 0.00022616083\n",
      "Epoch 83, change: 0.00021000589\n",
      "Epoch 84, change: 0.00017112843\n",
      "Epoch 85, change: 0.00025884103\n",
      "Epoch 86, change: 0.00018370476\n",
      "Epoch 87, change: 0.00020540209\n",
      "Epoch 88, change: 0.0001637742\n",
      "Epoch 89, change: 0.00019885419\n",
      "Epoch 90, change: 0.00016242408\n",
      "Epoch 91, change: 0.00018869904\n",
      "Epoch 92, change: 0.00020174467\n",
      "Epoch 93, change: 0.00018641923\n",
      "Epoch 94, change: 0.00020126946\n",
      "Epoch 95, change: 0.0001748466\n",
      "Epoch 96, change: 0.00015974914\n",
      "Epoch 97, change: 0.00022339734\n",
      "Epoch 98, change: 0.00014928899\n",
      "Epoch 99, change: 0.00025178818\n",
      "Epoch 100, change: 0.00015898961\n",
      "Epoch 101, change: 0.00021374243\n",
      "Epoch 102, change: 0.00017734703\n",
      "Epoch 103, change: 0.00016049967\n",
      "Epoch 104, change: 0.00020257379\n",
      "Epoch 105, change: 0.00021036978\n",
      "Epoch 106, change: 0.00018364757\n",
      "Epoch 107, change: 0.0001883048\n",
      "Epoch 108, change: 0.00027977652\n",
      "Epoch 109, change: 0.00020133595\n",
      "Epoch 110, change: 0.00018444646\n",
      "Epoch 111, change: 0.00019034467\n",
      "Epoch 112, change: 0.00017520446\n",
      "Epoch 113, change: 0.00023308978\n",
      "Epoch 114, change: 0.00025530183\n",
      "Epoch 115, change: 0.00016362558\n",
      "Epoch 116, change: 0.00019752749\n",
      "Epoch 117, change: 0.00018505285\n",
      "Epoch 118, change: 0.00018781998\n",
      "Epoch 119, change: 0.0001972907\n",
      "Epoch 120, change: 0.00016142237\n",
      "Epoch 121, change: 0.0001904294\n",
      "Epoch 122, change: 0.00018863998\n",
      "Epoch 123, change: 0.00027148356\n",
      "Epoch 124, change: 0.00020179954\n",
      "Epoch 125, change: 0.00018910794\n",
      "Epoch 126, change: 0.00019141068\n",
      "Epoch 127, change: 0.00016632109\n",
      "Epoch 128, change: 0.00019708014\n",
      "Epoch 129, change: 0.00019498578\n",
      "Epoch 130, change: 0.00018492834\n",
      "Epoch 131, change: 0.00019296228\n",
      "Epoch 132, change: 0.00017224179\n",
      "Epoch 133, change: 0.00018397313\n",
      "Epoch 134, change: 0.00022587129\n",
      "Epoch 135, change: 0.00020196011\n",
      "Epoch 136, change: 0.00024949078\n",
      "Epoch 137, change: 0.00019640871\n",
      "Epoch 138, change: 0.00019158649\n",
      "Epoch 139, change: 0.00016027292\n",
      "Epoch 140, change: 0.00019464119\n",
      "Epoch 141, change: 0.00017794015\n",
      "Epoch 142, change: 0.00015837143\n",
      "Epoch 143, change: 0.00018062565\n",
      "Epoch 144, change: 0.00019091986\n",
      "Epoch 145, change: 0.00019309699\n",
      "Epoch 146, change: 0.0002595229\n",
      "Epoch 147, change: 0.00018804186\n",
      "Epoch 148, change: 0.00016450704\n",
      "Epoch 149, change: 0.0001959948\n",
      "Epoch 150, change: 0.00015739736\n",
      "Epoch 151, change: 0.00017642\n",
      "Epoch 152, change: 0.00020350957\n",
      "Epoch 153, change: 0.00017867201\n",
      "Epoch 154, change: 0.00017871091\n",
      "Epoch 155, change: 0.00017509089\n",
      "Epoch 156, change: 0.00017456447\n",
      "Epoch 157, change: 0.00019208889\n",
      "Epoch 158, change: 0.00015550679\n",
      "Epoch 159, change: 0.00023151634\n",
      "Epoch 160, change: 0.00018479925\n",
      "Epoch 161, change: 0.00026584935\n",
      "Epoch 162, change: 0.00016239489\n",
      "Epoch 163, change: 0.00019332398\n",
      "Epoch 164, change: 0.00018224184\n",
      "Epoch 165, change: 0.00018345768\n",
      "Epoch 166, change: 0.00019885029\n",
      "Epoch 167, change: 0.00017335884\n",
      "Epoch 168, change: 0.00017017675\n",
      "Epoch 169, change: 0.00018316747\n",
      "Epoch 170, change: 0.00018779187\n",
      "Epoch 171, change: 0.00018696333\n",
      "Epoch 172, change: 0.00026553267\n",
      "Epoch 173, change: 0.00021152446\n",
      "Epoch 174, change: 0.00021348847\n",
      "Epoch 175, change: 0.00018515087\n",
      "Epoch 176, change: 0.0001441989\n",
      "Epoch 177, change: 0.00020555482\n",
      "Epoch 178, change: 0.0002615074\n",
      "Epoch 179, change: 0.00018017528\n",
      "Epoch 180, change: 0.0001798138\n",
      "Epoch 181, change: 0.00016854447\n",
      "Epoch 182, change: 0.00016148803\n",
      "Epoch 183, change: 0.00017490984\n",
      "Epoch 184, change: 0.00019518242\n",
      "Epoch 185, change: 0.00014651778\n",
      "Epoch 186, change: 0.00023103165\n",
      "Epoch 187, change: 0.00018664324\n",
      "Epoch 188, change: 0.00017971096\n",
      "Epoch 189, change: 0.00026082268\n",
      "Epoch 190, change: 0.00016412517\n",
      "Epoch 191, change: 0.00015471617\n",
      "Epoch 192, change: 0.00019016767\n",
      "Epoch 193, change: 0.0001703468\n",
      "Epoch 194, change: 0.00016000074\n",
      "Epoch 195, change: 0.00020732195\n",
      "Epoch 196, change: 0.00018090475\n",
      "Epoch 197, change: 0.00019154036\n",
      "Epoch 198, change: 0.00017529246\n",
      "Epoch 199, change: 0.00021099253\n",
      "Epoch 200, change: 0.00017592801\n",
      "Epoch 201, change: 0.00026001336\n",
      "Epoch 202, change: 0.00018209171\n",
      "Epoch 203, change: 0.0001976233\n",
      "Epoch 204, change: 0.00016940913\n",
      "Epoch 205, change: 0.00015231411\n",
      "Epoch 206, change: 0.00019607031\n",
      "Epoch 207, change: 0.00017856617\n",
      "Epoch 208, change: 0.00015750932\n",
      "Epoch 209, change: 0.00018912014\n",
      "Epoch 210, change: 0.00019765206\n",
      "Epoch 211, change: 0.00019081665\n",
      "Epoch 212, change: 0.00020337492\n",
      "Epoch 213, change: 0.00019638019\n",
      "Epoch 214, change: 0.00016928617\n",
      "Epoch 215, change: 0.00016384065\n",
      "Epoch 216, change: 0.00020313726\n",
      "Epoch 217, change: 0.00017194136\n",
      "Epoch 218, change: 0.00026362139\n",
      "Epoch 219, change: 0.00020025356\n",
      "Epoch 220, change: 0.0001800337\n",
      "Epoch 221, change: 0.00018837651\n",
      "Epoch 222, change: 0.00017512316\n",
      "Epoch 223, change: 0.00021102576\n",
      "Epoch 224, change: 0.00021410112\n",
      "Epoch 225, change: 0.00025301526\n",
      "Epoch 226, change: 0.00019665243\n",
      "Epoch 227, change: 0.0001910877\n",
      "Epoch 228, change: 0.000150883\n",
      "Epoch 229, change: 0.00020393441\n",
      "Epoch 230, change: 0.00017223957\n",
      "Epoch 231, change: 0.00022718332\n",
      "Epoch 232, change: 0.0002623756\n",
      "Epoch 233, change: 0.00017255277\n",
      "Epoch 234, change: 0.0001895396\n",
      "Epoch 235, change: 0.00019289648\n",
      "Epoch 236, change: 0.00018931073\n",
      "Epoch 237, change: 0.00025951452\n",
      "Epoch 238, change: 0.0001761418\n",
      "Epoch 239, change: 0.00019089844\n",
      "Epoch 240, change: 0.00017740067\n",
      "Epoch 241, change: 0.00015076394\n",
      "Epoch 242, change: 0.00022835446\n",
      "Epoch 243, change: 0.00025388304\n",
      "Epoch 244, change: 0.00015105156\n",
      "Epoch 245, change: 0.00020459463\n",
      "Epoch 246, change: 0.00018669819\n",
      "Epoch 247, change: 0.00019439105\n",
      "Epoch 248, change: 0.00019516118\n",
      "Epoch 249, change: 0.00019067708\n",
      "Epoch 250, change: 0.00016066528\n",
      "Epoch 251, change: 0.00021789785\n",
      "Epoch 252, change: 0.00015243946\n",
      "Epoch 253, change: 0.00021627337\n",
      "Epoch 254, change: 0.00025970847\n",
      "Epoch 255, change: 0.00022657927\n",
      "Epoch 256, change: 0.00018867297\n",
      "Epoch 257, change: 0.00020659139\n",
      "Epoch 258, change: 0.00015324555\n",
      "Epoch 259, change: 0.00019554733\n",
      "Epoch 260, change: 0.00018245346\n",
      "Epoch 261, change: 0.00020275566\n",
      "Epoch 262, change: 0.00019070801\n",
      "Epoch 263, change: 0.00025493529\n",
      "Epoch 264, change: 0.00020160867\n",
      "Epoch 265, change: 0.00015279517\n",
      "Epoch 266, change: 0.00017994376\n",
      "Epoch 267, change: 0.00020560516\n",
      "Epoch 268, change: 0.00018597017\n",
      "Epoch 269, change: 0.00019966\n",
      "Epoch 270, change: 0.00020912116\n",
      "Epoch 271, change: 0.00017895419\n",
      "Epoch 272, change: 0.00018974468\n",
      "Epoch 273, change: 0.00018532513\n",
      "Epoch 274, change: 0.00015627462\n",
      "Epoch 275, change: 0.0002051117\n",
      "Epoch 276, change: 0.00019336831\n",
      "Epoch 277, change: 0.00018579041\n",
      "Epoch 278, change: 0.00018905374\n",
      "Epoch 279, change: 0.00019612713\n",
      "Epoch 280, change: 0.00019832619\n",
      "Epoch 281, change: 0.00021974639\n",
      "Epoch 282, change: 0.00019252439\n",
      "Epoch 283, change: 0.00019228579\n",
      "Epoch 284, change: 0.00019402879\n",
      "Epoch 285, change: 0.00026720617\n",
      "Epoch 286, change: 0.00018122759\n",
      "Epoch 287, change: 0.00018916928\n",
      "Epoch 288, change: 0.00015943582\n",
      "Epoch 289, change: 0.00021642212\n",
      "Epoch 290, change: 0.00019537126\n",
      "Epoch 291, change: 0.00019355369\n",
      "Epoch 292, change: 0.00014599937\n",
      "Epoch 293, change: 0.0001976754\n",
      "Epoch 294, change: 0.00021667767\n",
      "Epoch 295, change: 0.00015477679\n",
      "Epoch 296, change: 0.00019642549\n",
      "Epoch 297, change: 0.00017059421\n",
      "Epoch 298, change: 0.00018981271\n",
      "Epoch 299, change: 0.00019906904\n",
      "Epoch 300, change: 0.00021249567\n",
      "Epoch 301, change: 0.00019710907\n",
      "Epoch 302, change: 0.0002033764\n",
      "Epoch 303, change: 0.00019142404\n",
      "Epoch 304, change: 0.00025791285\n",
      "Epoch 305, change: 0.00020600899\n",
      "Epoch 306, change: 0.00018129806\n",
      "Epoch 307, change: 0.0001915334\n",
      "Epoch 308, change: 0.00015845158\n",
      "Epoch 309, change: 0.00020883945\n",
      "Epoch 310, change: 0.00026565747\n",
      "Epoch 311, change: 0.00015402854\n",
      "Epoch 312, change: 0.00020010598\n",
      "Epoch 313, change: 0.00019952696\n",
      "Epoch 314, change: 0.000169327\n",
      "Epoch 315, change: 0.00023486472\n",
      "Epoch 316, change: 0.00019603473\n",
      "Epoch 317, change: 0.00019753254\n",
      "Epoch 318, change: 0.00018917742\n",
      "Epoch 319, change: 0.00019925571\n",
      "Epoch 320, change: 0.00020667003\n",
      "Epoch 321, change: 0.0001862046\n",
      "Epoch 322, change: 0.00018512318\n",
      "Epoch 323, change: 0.00020861872\n",
      "Epoch 324, change: 0.0001688496\n",
      "Epoch 325, change: 0.00018403931\n",
      "Epoch 326, change: 0.00019972523\n",
      "Epoch 327, change: 0.00019871398\n",
      "Epoch 328, change: 0.00013502364\n",
      "Epoch 329, change: 0.00019086282\n",
      "Epoch 330, change: 0.0002104625\n",
      "Epoch 331, change: 0.0002054569\n",
      "Epoch 332, change: 0.00022095986\n",
      "Epoch 333, change: 0.00020039792\n",
      "Epoch 334, change: 0.00020914419\n",
      "Epoch 335, change: 0.0001535022\n",
      "Epoch 336, change: 0.00026231643\n",
      "Epoch 337, change: 0.00020545544\n",
      "Epoch 338, change: 0.00015812693\n",
      "Epoch 339, change: 0.00019605739\n",
      "Epoch 340, change: 0.00019656168\n",
      "Epoch 341, change: 0.00013700876\n",
      "Epoch 342, change: 0.0002150338\n",
      "Epoch 343, change: 0.00025896498\n",
      "Epoch 344, change: 0.00018135094\n",
      "Epoch 345, change: 0.00020143772\n",
      "Epoch 346, change: 0.00019500358\n",
      "Epoch 347, change: 0.0001883201\n",
      "Epoch 348, change: 0.00020270213\n",
      "Epoch 349, change: 0.0001917967\n",
      "Epoch 350, change: 0.00015026155\n",
      "Epoch 351, change: 0.00020473718\n",
      "Epoch 352, change: 0.00019613138\n",
      "Epoch 353, change: 0.00019771908\n",
      "Epoch 354, change: 0.00025284049\n",
      "Epoch 355, change: 0.00019777467\n",
      "Epoch 356, change: 0.00020584054\n",
      "Epoch 357, change: 0.00019946897\n",
      "Epoch 358, change: 0.00019904392\n",
      "Epoch 359, change: 0.00025890075\n",
      "Epoch 360, change: 0.00018773024\n",
      "Epoch 361, change: 0.00015422044\n",
      "Epoch 362, change: 0.00020231324\n",
      "Epoch 363, change: 0.00019259422\n",
      "Epoch 364, change: 0.00022245759\n",
      "Epoch 365, change: 0.00021199472\n",
      "Epoch 366, change: 0.00019124024\n",
      "Epoch 367, change: 0.00025754404\n",
      "Epoch 368, change: 0.00020427852\n",
      "Epoch 369, change: 0.00016779351\n",
      "Epoch 370, change: 0.00018397518\n",
      "Epoch 371, change: 0.00020908045\n",
      "Epoch 372, change: 0.00014883374\n",
      "Epoch 373, change: 0.00021065208\n",
      "Epoch 374, change: 0.00021310805\n",
      "Epoch 375, change: 0.00014975827\n",
      "Epoch 376, change: 0.00021631535\n",
      "Epoch 377, change: 0.00021260665\n",
      "Epoch 378, change: 0.00020850995\n",
      "Epoch 379, change: 0.00020662238\n",
      "Epoch 380, change: 0.00019946757\n",
      "Epoch 381, change: 0.00021743131\n",
      "Epoch 382, change: 0.00016836343\n",
      "Epoch 383, change: 0.00017551401\n",
      "Epoch 384, change: 0.00019554365\n",
      "Epoch 385, change: 0.0001451787\n",
      "Epoch 386, change: 0.00020030503\n",
      "Epoch 387, change: 0.0001519471\n",
      "Epoch 388, change: 0.00020840784\n",
      "Epoch 389, change: 0.00026070519\n",
      "Epoch 390, change: 0.00018717833\n",
      "Epoch 391, change: 0.00018151918\n",
      "Epoch 392, change: 0.00021514422\n",
      "Epoch 393, change: 0.00019433248\n",
      "Epoch 394, change: 0.00020045413\n",
      "Epoch 395, change: 0.00021105904\n",
      "Epoch 396, change: 0.00016854951\n",
      "Epoch 397, change: 0.00020199288\n",
      "Epoch 398, change: 0.00021696893\n",
      "Epoch 399, change: 0.0001368857\n",
      "Epoch 400, change: 0.00025344387\n",
      "Epoch 401, change: 0.00019770178\n",
      "Epoch 402, change: 0.00013491811\n",
      "Epoch 403, change: 0.00020528416\n",
      "Epoch 404, change: 0.00020420931\n",
      "Epoch 405, change: 0.00016876719\n",
      "Epoch 406, change: 0.00020446733\n",
      "Epoch 407, change: 0.00018812057\n",
      "Epoch 408, change: 0.00020064581\n",
      "Epoch 409, change: 0.00020532603\n",
      "Epoch 410, change: 0.00020592721\n",
      "Epoch 411, change: 0.00013052759\n",
      "Epoch 412, change: 0.0002095728\n",
      "Epoch 413, change: 0.00020564058\n",
      "Epoch 414, change: 0.00019626458\n",
      "Epoch 415, change: 0.00020826499\n",
      "Epoch 416, change: 0.00020514411\n",
      "Epoch 417, change: 0.00019721105\n",
      "Epoch 418, change: 0.00015992155\n",
      "Epoch 419, change: 0.00020569055\n",
      "Epoch 420, change: 0.00019877916\n",
      "Epoch 421, change: 0.00022094285\n",
      "Epoch 422, change: 0.00019493228\n",
      "Epoch 423, change: 0.00019633537\n",
      "Epoch 424, change: 0.00020220093\n",
      "Epoch 425, change: 0.0002127342\n",
      "Epoch 426, change: 0.00012946957\n",
      "Epoch 427, change: 0.00021229168\n",
      "Epoch 428, change: 0.00020678679\n",
      "Epoch 429, change: 0.00025827732\n",
      "Epoch 430, change: 0.00021536091\n",
      "Epoch 431, change: 0.00017314815\n",
      "Epoch 432, change: 0.00018781265\n",
      "Epoch 433, change: 0.00020417877\n",
      "Epoch 434, change: 0.00018749064\n",
      "Epoch 435, change: 0.00021668602\n",
      "Epoch 436, change: 0.00019984167\n",
      "Epoch 437, change: 0.00021320389\n",
      "Epoch 438, change: 0.00021529791\n",
      "Epoch 439, change: 0.000194493\n",
      "Epoch 440, change: 0.00023676228\n",
      "Epoch 441, change: 0.00020764927\n",
      "Epoch 442, change: 0.00020579957\n",
      "Epoch 443, change: 0.0002144536\n",
      "Epoch 444, change: 0.00020708982\n",
      "Epoch 445, change: 0.0002053955\n",
      "Epoch 446, change: 0.00021509377\n",
      "Epoch 447, change: 0.00020113251\n",
      "Epoch 448, change: 0.00017313242\n",
      "Epoch 449, change: 0.00025399696\n",
      "Epoch 450, change: 0.00020920599\n",
      "Epoch 451, change: 0.00026119137\n",
      "Epoch 452, change: 0.00020805125\n",
      "Epoch 453, change: 0.00021599495\n",
      "Epoch 454, change: 0.00019400893\n",
      "Epoch 455, change: 0.00021019124\n",
      "Epoch 456, change: 0.00020806651\n",
      "Epoch 457, change: 0.00018440599\n",
      "Epoch 458, change: 0.00019924491\n",
      "Epoch 459, change: 0.00021707015\n",
      "Epoch 460, change: 0.00019619579\n",
      "Epoch 461, change: 0.00022948276\n",
      "Epoch 462, change: 0.00020626301\n",
      "Epoch 463, change: 0.00015137541\n",
      "Epoch 464, change: 0.00020637679\n",
      "Epoch 465, change: 0.00026373731\n",
      "Epoch 466, change: 0.00018346201\n",
      "Epoch 467, change: 0.00020584116\n",
      "Epoch 468, change: 0.0002055401\n",
      "Epoch 469, change: 0.00015963028\n",
      "Epoch 470, change: 0.0002111833\n",
      "Epoch 471, change: 0.00018609458\n",
      "Epoch 472, change: 0.0002598328\n",
      "Epoch 473, change: 0.00021424957\n",
      "Epoch 474, change: 0.00015273457\n",
      "Epoch 475, change: 0.00020129148\n",
      "Epoch 476, change: 0.00021614115\n",
      "Epoch 477, change: 0.00018634321\n",
      "Epoch 478, change: 0.00020363939\n",
      "Epoch 479, change: 0.00022837594\n",
      "Epoch 480, change: 0.00019102659\n",
      "Epoch 481, change: 0.00020652825\n",
      "Epoch 482, change: 0.0002065072\n",
      "Epoch 483, change: 0.00020158832\n",
      "Epoch 484, change: 0.00021061163\n",
      "Epoch 485, change: 0.00020569601\n",
      "Epoch 486, change: 0.00018733155\n",
      "Epoch 487, change: 0.00021249958\n",
      "Epoch 488, change: 0.00027396201\n",
      "Epoch 489, change: 0.00025694194\n",
      "Epoch 490, change: 0.00021457425\n",
      "Epoch 491, change: 0.000207923\n",
      "Epoch 492, change: 0.00019770817\n",
      "Epoch 493, change: 0.00019932786\n",
      "Epoch 494, change: 0.00020974907\n",
      "Epoch 495, change: 0.00025319631\n",
      "Epoch 496, change: 0.00021804433\n",
      "Epoch 497, change: 0.00019407521\n",
      "Epoch 498, change: 0.00019123257\n",
      "Epoch 499, change: 0.00018287466\n",
      "max_iter reached after 167 secondsEpoch 500, change: 0.00026726889\n",
      "\n",
      "\n",
      "Accuracy: 0.7533\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Buzzfeed News       0.57      0.54      0.55      1000\n",
      "               CNN       0.65      0.71      0.68      1000\n",
      "         Economist       0.84      0.93      0.89      1000\n",
      "          Fox News       0.78      0.69      0.73      1000\n",
      "            People       0.81      0.89      0.85      1000\n",
      "          Politico       0.64      0.72      0.67      1000\n",
      "           Reuters       0.85      0.84      0.85      1000\n",
      "          The Hill       0.85      0.75      0.79      1000\n",
      "The New York Times       0.77      0.68      0.72      1000\n",
      "              Vice       0.78      0.79      0.79      1000\n",
      "\n",
      "          accuracy                           0.75     10000\n",
      "         macro avg       0.75      0.75      0.75     10000\n",
      "      weighted avg       0.75      0.75      0.75     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "print(\"➜ fitting multinomial logistic regression …\")\n",
    "\n",
    "# Fit the model\n",
    "clf = LogisticRegression(\n",
    "    max_iter=500,\n",
    "    n_jobs=-1,\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"saga\",   # <── swap lbfgs → saga for built‑in progress\n",
    "    verbose=2        # 0 = silent, 1 = compact, 2 = detailed\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=lbl.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cd5bb_row0_col1, #T_cd5bb_row0_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 53.9%, transparent 53.9%);\n",
       "}\n",
       "#T_cd5bb_row0_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 57.0%, transparent 57.0%);\n",
       "}\n",
       "#T_cd5bb_row0_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 55.4%, transparent 55.4%);\n",
       "}\n",
       "#T_cd5bb_row1_col1, #T_cd5bb_row1_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 70.8%, transparent 70.8%);\n",
       "}\n",
       "#T_cd5bb_row1_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 65.2%, transparent 65.2%);\n",
       "}\n",
       "#T_cd5bb_row1_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 67.9%, transparent 67.9%);\n",
       "}\n",
       "#T_cd5bb_row2_col1, #T_cd5bb_row2_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 93.2%, transparent 93.2%);\n",
       "}\n",
       "#T_cd5bb_row2_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 84.4%, transparent 84.4%);\n",
       "}\n",
       "#T_cd5bb_row2_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 88.6%, transparent 88.6%);\n",
       "}\n",
       "#T_cd5bb_row3_col1, #T_cd5bb_row3_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 69.5%, transparent 69.5%);\n",
       "}\n",
       "#T_cd5bb_row3_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 77.7%, transparent 77.7%);\n",
       "}\n",
       "#T_cd5bb_row3_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 73.4%, transparent 73.4%);\n",
       "}\n",
       "#T_cd5bb_row4_col1, #T_cd5bb_row4_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 89.2%, transparent 89.2%);\n",
       "}\n",
       "#T_cd5bb_row4_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 81.5%, transparent 81.5%);\n",
       "}\n",
       "#T_cd5bb_row4_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 85.2%, transparent 85.2%);\n",
       "}\n",
       "#T_cd5bb_row5_col1, #T_cd5bb_row5_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 71.7%, transparent 71.7%);\n",
       "}\n",
       "#T_cd5bb_row5_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 63.7%, transparent 63.7%);\n",
       "}\n",
       "#T_cd5bb_row5_col4, #T_cd5bb_row8_col1, #T_cd5bb_row8_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 67.5%, transparent 67.5%);\n",
       "}\n",
       "#T_cd5bb_row6_col1, #T_cd5bb_row6_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 84.0%, transparent 84.0%);\n",
       "}\n",
       "#T_cd5bb_row6_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 85.1%, transparent 85.1%);\n",
       "}\n",
       "#T_cd5bb_row6_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 84.5%, transparent 84.5%);\n",
       "}\n",
       "#T_cd5bb_row7_col1, #T_cd5bb_row7_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 74.7%, transparent 74.7%);\n",
       "}\n",
       "#T_cd5bb_row7_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 84.8%, transparent 84.8%);\n",
       "}\n",
       "#T_cd5bb_row7_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 79.4%, transparent 79.4%);\n",
       "}\n",
       "#T_cd5bb_row8_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 77.1%, transparent 77.1%);\n",
       "}\n",
       "#T_cd5bb_row8_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 72.0%, transparent 72.0%);\n",
       "}\n",
       "#T_cd5bb_row9_col1, #T_cd5bb_row9_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 78.8%, transparent 78.8%);\n",
       "}\n",
       "#T_cd5bb_row9_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 78.3%, transparent 78.3%);\n",
       "}\n",
       "#T_cd5bb_row9_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 78.5%, transparent 78.5%);\n",
       "}\n",
       "#T_cd5bb_row10_col1, #T_cd5bb_row10_col3 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 75.3%, transparent 75.3%);\n",
       "}\n",
       "#T_cd5bb_row10_col2 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 75.5%, transparent 75.5%);\n",
       "}\n",
       "#T_cd5bb_row10_col4 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #66c2a5 75.2%, transparent 75.2%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cd5bb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cd5bb_level0_col0\" class=\"col_heading level0 col0\" >Publication</th>\n",
       "      <th id=\"T_cd5bb_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_cd5bb_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_cd5bb_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_cd5bb_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cd5bb_row0_col0\" class=\"data row0 col0\" >Buzzfeed News</td>\n",
       "      <td id=\"T_cd5bb_row0_col1\" class=\"data row0 col1\" >0.539000</td>\n",
       "      <td id=\"T_cd5bb_row0_col2\" class=\"data row0 col2\" >0.570000</td>\n",
       "      <td id=\"T_cd5bb_row0_col3\" class=\"data row0 col3\" >0.539000</td>\n",
       "      <td id=\"T_cd5bb_row0_col4\" class=\"data row0 col4\" >0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cd5bb_row1_col0\" class=\"data row1 col0\" >CNN</td>\n",
       "      <td id=\"T_cd5bb_row1_col1\" class=\"data row1 col1\" >0.708000</td>\n",
       "      <td id=\"T_cd5bb_row1_col2\" class=\"data row1 col2\" >0.652000</td>\n",
       "      <td id=\"T_cd5bb_row1_col3\" class=\"data row1 col3\" >0.708000</td>\n",
       "      <td id=\"T_cd5bb_row1_col4\" class=\"data row1 col4\" >0.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cd5bb_row2_col0\" class=\"data row2 col0\" >Economist</td>\n",
       "      <td id=\"T_cd5bb_row2_col1\" class=\"data row2 col1\" >0.932000</td>\n",
       "      <td id=\"T_cd5bb_row2_col2\" class=\"data row2 col2\" >0.844000</td>\n",
       "      <td id=\"T_cd5bb_row2_col3\" class=\"data row2 col3\" >0.932000</td>\n",
       "      <td id=\"T_cd5bb_row2_col4\" class=\"data row2 col4\" >0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cd5bb_row3_col0\" class=\"data row3 col0\" >Fox News</td>\n",
       "      <td id=\"T_cd5bb_row3_col1\" class=\"data row3 col1\" >0.695000</td>\n",
       "      <td id=\"T_cd5bb_row3_col2\" class=\"data row3 col2\" >0.777000</td>\n",
       "      <td id=\"T_cd5bb_row3_col3\" class=\"data row3 col3\" >0.695000</td>\n",
       "      <td id=\"T_cd5bb_row3_col4\" class=\"data row3 col4\" >0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cd5bb_row4_col0\" class=\"data row4 col0\" >People</td>\n",
       "      <td id=\"T_cd5bb_row4_col1\" class=\"data row4 col1\" >0.892000</td>\n",
       "      <td id=\"T_cd5bb_row4_col2\" class=\"data row4 col2\" >0.815000</td>\n",
       "      <td id=\"T_cd5bb_row4_col3\" class=\"data row4 col3\" >0.892000</td>\n",
       "      <td id=\"T_cd5bb_row4_col4\" class=\"data row4 col4\" >0.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_cd5bb_row5_col0\" class=\"data row5 col0\" >Politico</td>\n",
       "      <td id=\"T_cd5bb_row5_col1\" class=\"data row5 col1\" >0.717000</td>\n",
       "      <td id=\"T_cd5bb_row5_col2\" class=\"data row5 col2\" >0.637000</td>\n",
       "      <td id=\"T_cd5bb_row5_col3\" class=\"data row5 col3\" >0.717000</td>\n",
       "      <td id=\"T_cd5bb_row5_col4\" class=\"data row5 col4\" >0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_cd5bb_row6_col0\" class=\"data row6 col0\" >Reuters</td>\n",
       "      <td id=\"T_cd5bb_row6_col1\" class=\"data row6 col1\" >0.840000</td>\n",
       "      <td id=\"T_cd5bb_row6_col2\" class=\"data row6 col2\" >0.851000</td>\n",
       "      <td id=\"T_cd5bb_row6_col3\" class=\"data row6 col3\" >0.840000</td>\n",
       "      <td id=\"T_cd5bb_row6_col4\" class=\"data row6 col4\" >0.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_cd5bb_row7_col0\" class=\"data row7 col0\" >The Hill</td>\n",
       "      <td id=\"T_cd5bb_row7_col1\" class=\"data row7 col1\" >0.747000</td>\n",
       "      <td id=\"T_cd5bb_row7_col2\" class=\"data row7 col2\" >0.848000</td>\n",
       "      <td id=\"T_cd5bb_row7_col3\" class=\"data row7 col3\" >0.747000</td>\n",
       "      <td id=\"T_cd5bb_row7_col4\" class=\"data row7 col4\" >0.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_cd5bb_row8_col0\" class=\"data row8 col0\" >The New York Times</td>\n",
       "      <td id=\"T_cd5bb_row8_col1\" class=\"data row8 col1\" >0.675000</td>\n",
       "      <td id=\"T_cd5bb_row8_col2\" class=\"data row8 col2\" >0.771000</td>\n",
       "      <td id=\"T_cd5bb_row8_col3\" class=\"data row8 col3\" >0.675000</td>\n",
       "      <td id=\"T_cd5bb_row8_col4\" class=\"data row8 col4\" >0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_cd5bb_row9_col0\" class=\"data row9 col0\" >Vice</td>\n",
       "      <td id=\"T_cd5bb_row9_col1\" class=\"data row9 col1\" >0.788000</td>\n",
       "      <td id=\"T_cd5bb_row9_col2\" class=\"data row9 col2\" >0.783000</td>\n",
       "      <td id=\"T_cd5bb_row9_col3\" class=\"data row9 col3\" >0.788000</td>\n",
       "      <td id=\"T_cd5bb_row9_col4\" class=\"data row9 col4\" >0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd5bb_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_cd5bb_row10_col0\" class=\"data row10 col0\" >**Top-Line**</td>\n",
       "      <td id=\"T_cd5bb_row10_col1\" class=\"data row10 col1\" >0.753000</td>\n",
       "      <td id=\"T_cd5bb_row10_col2\" class=\"data row10 col2\" >0.755000</td>\n",
       "      <td id=\"T_cd5bb_row10_col3\" class=\"data row10 col3\" >0.753000</td>\n",
       "      <td id=\"T_cd5bb_row10_col4\" class=\"data row10 col4\" >0.752000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x43689e0c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Compute per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred, labels=range(len(lbl.classes_)), zero_division=0\n",
    ")\n",
    "\n",
    "# Compute accuracy per class (same as before using confusion matrix)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=range(len(lbl.classes_)))\n",
    "hits = cm.diagonal()\n",
    "total_true = cm.sum(axis=1)\n",
    "acc_per_class = hits / total_true\n",
    "\n",
    "# Create a DataFrame with all metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Publication\": lbl.classes_,\n",
    "    \"Accuracy\": acc_per_class.round(3),\n",
    "    \"Precision\": precision.round(3),\n",
    "    \"Recall\": recall.round(3),\n",
    "    \"F1\": f1.round(3)\n",
    "}).sort_values(\"Publication\")\n",
    "\n",
    "# Add a Top-Line summary row\n",
    "topline_precision, topline_recall, topline_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "topline_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "topline = pd.DataFrame([{\n",
    "    \"Publication\": \"**Top-Line**\",\n",
    "    \"Accuracy\": round(topline_accuracy, 3),\n",
    "    \"Precision\": round(topline_precision, 3),\n",
    "    \"Recall\": round(topline_recall, 3),\n",
    "    \"F1\": round(topline_f1, 3)\n",
    "}])\n",
    "\n",
    "# Append Top-Line row\n",
    "metrics_df = pd.concat([metrics_df, topline], ignore_index=True)\n",
    "\n",
    "# Display\n",
    "display(metrics_df.style.bar(subset=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\"], vmin=0, vmax=1, color='#66c2a5'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

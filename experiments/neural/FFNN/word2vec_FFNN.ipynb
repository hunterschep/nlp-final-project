{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4nxQDqg-w9f"
      },
      "source": [
        "## News Source Attribution with FFNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTsmadia5EsP"
      },
      "source": [
        "Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kxTH1ps_fvO",
        "outputId": "8823cb9a-a505-4e73-f593-552e91fd904d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0CTzvi-B7uA5",
        "outputId": "e0abae40-9474-4131-a935-fe6921a15e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "71efc9308e414fee9b18dc213b8370eb",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install gensim\n",
        "!pip install numpy==1.24.3 --force-reinstall\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r0OLz_yf-w9k"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "import glob\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "import re\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "# import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h9dm9VSc3ITo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXUhp2n39xGT",
        "outputId": "44a8c8f0-585d-40e7-cffe-4a6b2d69cefc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbkgv5GN5M4C"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD3SlRDj5OlA",
        "outputId": "df5ccddd-d491-40de-fddb-9ece171d7844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = pd.read_csv('/content/all-the-news-2-1-SMALL-CLEANED.csv', encoding='latin-1')\n",
        "print(\"Data loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JJH9fuIGulL"
      },
      "source": [
        "Load embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgOJPfUl-S6y",
        "outputId": "ce07f001-6a91-4a98-e400-ade66288f108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-19 17:08:18--  https://github.com/eyaler/word2vec-slim/raw/refs/heads/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/eyaler/word2vec-slim/refs/heads/master/GoogleNews-vectors-negative300-SLIM.bin.gz [following]\n",
            "--2025-04-19 17:08:18--  https://media.githubusercontent.com/media/eyaler/word2vec-slim/refs/heads/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 276467217 (264M) [application/octet-stream]\n",
            "Saving to: ‘GoogleNews-vectors-negative300-SLIM.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>] 263.66M   257MB/s    in 1.0s    \n",
            "\n",
            "2025-04-19 17:08:23 (257 MB/s) - ‘GoogleNews-vectors-negative300-SLIM.bin.gz’ saved [276467217/276467217]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/eyaler/word2vec-slim/raw/refs/heads/master/GoogleNews-vectors-negative300-SLIM.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WLQ9gsq4-Q87"
      },
      "outputs": [],
      "source": [
        "bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin.gz\", binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yswc_E4Q9Gqq"
      },
      "source": [
        "Tokenize text by word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9Ccy84ZDYZei"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_articles = []\n",
        "test_articles = []\n",
        "train_labels = []\n",
        "test_labels = []\n",
        "train_count = 0\n",
        "test_count = 0\n",
        "for index, row in df.iterrows():\n",
        "    if row[\"split\"] == \"train\":\n",
        "      article = row[\"clean_article\"]\n",
        "      addme = [t.lower() for t in article.split()]# not in stoplist]\n",
        "      train_articles.append(addme)\n",
        "      train_labels.append(row[\"publication\"])\n",
        "\n",
        "    if row[\"split\"] == \"test\":\n",
        "      article = row[\"clean_article\"]\n",
        "      addme = [t.lower() for t in article.split()]# not in stoplist]\n",
        "      test_articles.append(addme)\n",
        "      test_labels.append(row[\"publication\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8vlwRNuBGeCU"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "  totvec = np.zeros(300)\n",
        "  for w in text:\n",
        "      if w.lower() in bigmodel:\n",
        "          totvec = totvec + bigmodel[w.lower()]\n",
        "  totvec = totvec / len(article)\n",
        "  return totvec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOxsnDQNG3vd"
      },
      "source": [
        "Build test + train data by getting average word embeddings of articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Uyjzxoir-RFT"
      },
      "outputs": [],
      "source": [
        "train_vectors = []\n",
        "\n",
        "for article in train_articles:\n",
        "    totvec = np.zeros(300)\n",
        "    for w in article:\n",
        "        if w.lower() in bigmodel:\n",
        "            totvec = totvec + bigmodel[w.lower()]\n",
        "    totvec = totvec / len(article)\n",
        "    train_vectors.append(totvec)\n",
        "\n",
        "\n",
        "test_vectors = []\n",
        "count = 0\n",
        "for article in test_articles:\n",
        "\n",
        "    totvec = np.zeros(300)\n",
        "    for w in article:\n",
        "        if w.lower() in bigmodel:\n",
        "            totvec = totvec + bigmodel[w.lower()]\n",
        "    totvec = totvec / len(article)\n",
        "    test_vectors.append(totvec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPr5j1bU69py",
        "outputId": "5b71b389-1eb1-441c-a1a3-7337de1cd6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The New York Times' 'The Hill' 'Reuters' 'People' 'CNN' 'Vice'\n",
            " 'Politico' 'Buzzfeed News' 'Economist' 'Fox News']\n",
            "{'The New York Times': 0, 'The Hill': 1, 'Reuters': 2, 'People': 3, 'CNN': 4, 'Vice': 5, 'Politico': 6, 'Buzzfeed News': 7, 'Economist': 8, 'Fox News': 9}\n"
          ]
        }
      ],
      "source": [
        "publications = df['publication'].unique()\n",
        "print(publications)\n",
        "publication_map = {publications[i]: i for i in range(len(publications))}\n",
        "print(publication_map)\n",
        "\n",
        "test_labels = [publication_map[p] for p in test_labels]\n",
        "train_labels = [publication_map[p] for p in train_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C4CPm2oG_G2"
      },
      "source": [
        "Run Neural Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM1_jiiW-w9n",
        "outputId": "1eb96cba-1960-457b-eb65-d2c518b9b6ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "Layers: (64, 32)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Multi-Layer Perceptron\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.37      0.41      1000\n",
            "           1       0.62      0.64      0.63      1000\n",
            "           2       0.68      0.71      0.69      1000\n",
            "           3       0.56      0.82      0.67      1000\n",
            "           4       0.32      0.18      0.23      1000\n",
            "           5       0.59      0.72      0.65      1000\n",
            "           6       0.42      0.52      0.47      1000\n",
            "           7       0.33      0.16      0.22      1000\n",
            "           8       0.80      0.82      0.81      1000\n",
            "           9       0.34      0.38      0.36      1000\n",
            "\n",
            "    accuracy                           0.53     10000\n",
            "   macro avg       0.51      0.53      0.51     10000\n",
            "weighted avg       0.51      0.53      0.51     10000\n",
            "\n",
            "\n",
            "\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "Layers: (16, 8)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Multi-Layer Perceptron\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1000\n",
            "           1       0.21      0.05      0.08      1000\n",
            "           2       0.28      0.79      0.41      1000\n",
            "           3       0.41      0.61      0.49      1000\n",
            "           4       0.06      0.01      0.02      1000\n",
            "           5       0.21      0.31      0.25      1000\n",
            "           6       0.21      0.01      0.01      1000\n",
            "           7       0.16      0.21      0.18      1000\n",
            "           8       0.19      0.47      0.27      1000\n",
            "           9       0.00      0.00      0.00      1000\n",
            "\n",
            "    accuracy                           0.25     10000\n",
            "   macro avg       0.17      0.25      0.17     10000\n",
            "weighted avg       0.17      0.25      0.17     10000\n",
            "\n",
            "\n",
            "\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "Layers: (128, 64)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Multi-Layer Perceptron\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.27      0.28      1000\n",
            "           1       0.49      0.63      0.55      1000\n",
            "           2       0.60      0.65      0.62      1000\n",
            "           3       0.57      0.76      0.65      1000\n",
            "           4       0.24      0.07      0.11      1000\n",
            "           5       0.53      0.71      0.61      1000\n",
            "           6       0.38      0.39      0.39      1000\n",
            "           7       0.33      0.18      0.24      1000\n",
            "           8       0.66      0.88      0.75      1000\n",
            "           9       0.33      0.26      0.29      1000\n",
            "\n",
            "    accuracy                           0.48     10000\n",
            "   macro avg       0.44      0.48      0.45     10000\n",
            "weighted avg       0.44      0.48      0.45     10000\n",
            "\n",
            "\n",
            "\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "Layers: (20, 10)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Multi-Layer Perceptron\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.38      0.40      1000\n",
            "           1       0.60      0.71      0.65      1000\n",
            "           2       0.62      0.74      0.67      1000\n",
            "           3       0.63      0.73      0.68      1000\n",
            "           4       0.19      0.07      0.10      1000\n",
            "           5       0.57      0.75      0.65      1000\n",
            "           6       0.44      0.52      0.48      1000\n",
            "           7       0.29      0.10      0.15      1000\n",
            "           8       0.75      0.88      0.81      1000\n",
            "           9       0.33      0.38      0.35      1000\n",
            "\n",
            "    accuracy                           0.53     10000\n",
            "   macro avg       0.48      0.53      0.49     10000\n",
            "weighted avg       0.48      0.53      0.49     10000\n",
            "\n",
            "\n",
            "\n",
            "#########################\n",
            "\n",
            "\n",
            "\n",
            "Layers: [128, 64, 32]\n",
            "\n",
            "Accuracy of Multi-Layer Perceptron\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.70      0.70      1000\n",
            "           1       0.84      0.79      0.81      1000\n",
            "           2       0.79      0.89      0.84      1000\n",
            "           3       0.74      0.84      0.79      1000\n",
            "           4       0.64      0.48      0.55      1000\n",
            "           5       0.83      0.75      0.79      1000\n",
            "           6       0.61      0.68      0.64      1000\n",
            "           7       0.50      0.55      0.53      1000\n",
            "           8       0.90      0.95      0.93      1000\n",
            "           9       0.63      0.54      0.58      1000\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.71     10000\n",
            "weighted avg       0.72      0.72      0.71     10000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "### NEURAL NET CLASSIFIER\n",
        "\n",
        "print(f\"\\n\\n\\nLayers: 128,64,32 \\n\")\n",
        "\n",
        "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(128, 64, 32), random_state=1)\n",
        "clf.fit(train_vectors, train_labels)\n",
        "predicted = clf.predict(test_vectors)\n",
        "print(\"Accuracy of Multi-Layer Perceptron\")\n",
        "print(metrics.classification_report(test_labels, predicted))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

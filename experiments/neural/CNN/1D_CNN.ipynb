{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c01d6958",
      "metadata": {
        "id": "c01d6958"
      },
      "source": [
        "# News Source Attribution With a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45c7a2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d45c7a2c",
        "outputId": "3ad663e5-c54c-4f22-8117-8998195f1885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1QoqGS6XE8BONzPvUQCVuGjaBdt_GaXlz\n",
            "From (redirected): https://drive.google.com/uc?id=1QoqGS6XE8BONzPvUQCVuGjaBdt_GaXlz&confirm=t&uuid=9b33e5ee-6193-45f3-badd-c5eb93071ece\n",
            "To: /content/all-the-news-2-1-SMALL-CLEANED.csv\n",
            "100%|██████████| 485M/485M [00:05<00:00, 93.0MB/s]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import gdown\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# File ID and destination\n",
        "file_id = \"1QoqGS6XE8BONzPvUQCVuGjaBdt_GaXlz\"\n",
        "output = \"all-the-news-2-1-SMALL-CLEANED.csv\"\n",
        "\n",
        "# Download from Google Drive\n",
        "gdown.download(id=file_id, output=output, quiet=False)\n",
        "data = pd.read_csv(\"all-the-news-2-1-SMALL-CLEANED.csv\")\n",
        "\n",
        "'''\n",
        "# Read in the data\n",
        "data = pd.read_csv(\"../../../data/all-the-news-2-1-SMALL-CLEANED.csv\")\n",
        "'''\n",
        "\n",
        "# Split into training and testing data\n",
        "train_data = data[data['split'] == 'train']\n",
        "test_data = data[data['split'] == 'test']\n",
        "\n",
        "# Extract text and labels\n",
        "train_texts = train_data['clean_article'].tolist()\n",
        "train_labels = train_data['publication'].tolist()\n",
        "test_texts = test_data['clean_article'].tolist()\n",
        "test_labels = test_data['publication'].tolist()\n",
        "\n",
        "# Encode labels\n",
        "# Use sklearn's LabelEncoder to convert the news sources into numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n",
        "\n",
        "# Get the number of unique labels (classes)\n",
        "num_labels = len(label_encoder.classes_)\n",
        "\n",
        "# Tokenize the text\n",
        "# Returns a list of lists of tokens\n",
        "def tokenize_texts(texts):\n",
        "    return [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "# Lists of lists of tokens for each article\n",
        "train_tokens = tokenize_texts(train_texts)\n",
        "test_tokens = tokenize_texts(test_texts)\n",
        "\n",
        "# Get the count of each token in the training data\n",
        "vocab = Counter(token for tokens in train_tokens for token in tokens)\n",
        "\n",
        "# Store the \"frequency ranking\" for each token\n",
        "vocab = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
        "\n",
        "# Convert tokens to indices\n",
        "# Returns a list of lists of the \"frequency ranking\" of each token in each article\n",
        "def tokens_to_indices(tokens, vocab):\n",
        "    return [[vocab.get(token, 0) for token in text] for text in tokens]\n",
        "\n",
        "# Lists of lists of tokens as \"frequency rankings\" for each article\n",
        "# These can be interpreted as dense vectors, with few zeros, where each vector entry is the frequency ranking for the corresponding token\n",
        "train_indices = tokens_to_indices(train_tokens, vocab)\n",
        "test_indices = tokens_to_indices(test_tokens, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a3b7ad",
      "metadata": {
        "id": "e9a3b7ad"
      },
      "source": [
        "### Padding\n",
        "At this point, the lengths of our dense article vectors are directly proportional to the article's word count. However, neural networks (including 1D CNNs) require fixed input sizes.\n",
        "\n",
        "We will use padding to ensure that our input sequences all have the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3dd529",
      "metadata": {
        "id": "ce3dd529"
      },
      "outputs": [],
      "source": [
        "# This function takes a list of lists (in other words, a list of dense vectors whose values correspond to respective tokens' frequency rankings)\n",
        "# We will reasonably shorten these vectors to have [max_len] elements\n",
        "# If a vector is shorter than [max_len], it is safe to append 0's to the end of the vector\n",
        "# Otherwise, we truncate it to the first [max_len] entries\n",
        "# Returns a PyTorch tensor, still a 2D matrix, or a list of vectors, but now with the vectors sharing a consistent length\n",
        "def pad(sequences, max_len):\n",
        "    return torch.tensor([seq[:max_len] + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences])\n",
        "\n",
        "# Store padded vectors\n",
        "max_len = 500\n",
        "train_padded = pad(train_indices, max_len)\n",
        "test_padded = pad(test_indices, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed0b185",
      "metadata": {
        "id": "6ed0b185"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Prepare data for PyTorch\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = NewsDataset(train_padded, train_labels)\n",
        "test_dataset = NewsDataset(test_padded, test_labels)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create a class for our 1D CNN model with an embedding layer\n",
        "# Use a PyTorch nn.Module to define the architecture of the 1D CNN model\n",
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "    # This function initializes the layers of the model\n",
        "    def __init__(self, vocab_size, embed_dim, num_labels):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Embedding layer\n",
        "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=5, stride=1, padding=2)  # 1D convolutional layer\n",
        "        self.relu = nn.ReLU()  # Non-linear activation function: ReLU\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)  # Max pooling layer: reduces the vector length by a factor of 2, keeping the most important features\n",
        "        self.fc = nn.Linear(128 * (max_len // 2), num_labels)  # Fully connected layer\n",
        "\n",
        "    # This function defines how input data flows through the CNN\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).permute(0, 2, 1)  # (batch_size, embed_dim, seq_len), convert the vectors into more dense vectors of size embed_dim\n",
        "        x = self.conv1(x)  # Apply the 1D convolution to extract local patterns (like n-grams, for example)\n",
        "        x = self.relu(x)  # Apply the non-linear activation function ReLU\n",
        "        x = self.pool(x)  # Reduce the vector length by a factor of 2\n",
        "        x = x.view(x.size(0), -1)  # \"Flatten\" the output into a 2D tensor\n",
        "        x = self.fc(x)\n",
        "        return x  # Return a tensor containing the predicted class scores for each sample in the batch\n",
        "\n",
        "# Initialize the model\n",
        "vocab_size = len(vocab) + 1  # Add 1 for padding index\n",
        "embed_dim = 100\n",
        "model = CNNClassifier(vocab_size, embed_dim, num_labels)\n",
        "\n",
        "# Train the model\n",
        "# Define loss and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()  # Using the cross-entropy loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam is a popular optimizer that adapts the learning rate and reduces the loss function\n",
        "\n",
        "# Training loop\n",
        "# 1 full pass through the training set is going to take about 1 hour on Colab\n",
        "# Increase [num_epochs] if time permits\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_times = []\n",
        "\n",
        "    # For each batch in the training set...\n",
        "    for i, (texts, labels) in enumerate(train_loader):\n",
        "        start = time.time()\n",
        "        optimizer.zero_grad()  # Reset the gradient from the previous step\n",
        "        outputs = model(texts)  # Get the predicted scores for each class\n",
        "        loss = loss_function(outputs, labels)  # Calculate the loss\n",
        "        loss.backward()  # Backpropogation: compute the gradient...\n",
        "        optimizer.step()  # ...and adjust weights based on the gradient\n",
        "        total_loss += loss.item()  # Keep track of the total loss\n",
        "        if len(batch_times) == 100:\n",
        "          print(f'average batch time = {(sum(batch_times) / len(batch_times)):.2f}s')\n",
        "          print(f'estimated epoch time = {(sum(batch_times) / len(batch_times)) * len(train_loader):.2f}s')\n",
        "        elif len(batch_times) < 100:\n",
        "          batch_times.append(time.time() - start)\n",
        "\n",
        "    # At the end of each epoch, print the average loss over all batches\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Stop tracking the gradient, not needed when evaluating\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
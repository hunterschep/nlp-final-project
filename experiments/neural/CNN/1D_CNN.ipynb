{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01d6958",
   "metadata": {},
   "source": [
    "# News Source Attribution With a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read in the data\n",
    "data = pd.read_csv(\"../../../data/all-the-news-2-1-SMALL-CLEANED.csv\")\n",
    "\n",
    "# Split into training and testing data\n",
    "train_data = data[data['split'] == 'train']\n",
    "test_data = data[data['split'] == 'test']\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = train_data['clean_article'].tolist()\n",
    "train_labels = train_data['publication'].tolist()\n",
    "test_texts = test_data['clean_article'].tolist()\n",
    "test_labels = test_data['publication'].tolist()\n",
    "\n",
    "# Encode labels\n",
    "# Use sklearn's LabelEncoder to convert the news sources into numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Get the number of unique labels (classes)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Tokenize the text\n",
    "# Returns a list of lists of tokens\n",
    "def tokenize_texts(texts):\n",
    "    return [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "# Lists of lists of tokens for each article\n",
    "train_tokens = tokenize_texts(train_texts)\n",
    "test_tokens = tokenize_texts(test_texts)\n",
    "\n",
    "# Get the count of each token in the training data\n",
    "vocab = Counter(token for tokens in train_tokens for token in tokens)\n",
    "\n",
    "# Store the \"frequency ranking\" for each token\n",
    "vocab = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "\n",
    "# Convert tokens to indices\n",
    "# Returns a list of lists of the \"frequency ranking\" of each token in each article\n",
    "def tokens_to_indices(tokens, vocab):\n",
    "    return [[vocab.get(token, 0) for token in text] for text in tokens]\n",
    "\n",
    "# Lists of lists of tokens as \"frequency rankings\" for each article\n",
    "# These can be interpreted as dense vectors, with few zeros, where each vector entry is the frequency ranking for the corresponding token\n",
    "train_indices = tokens_to_indices(train_tokens, vocab)\n",
    "test_indices = tokens_to_indices(test_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3b7ad",
   "metadata": {},
   "source": [
    "### Padding\n",
    "At this point, the lengths of our dense article vectors are directly proportional to the article's word count. However, neural networks (including 1D CNNs) require fixed input sizes.\n",
    "\n",
    "We will use padding to ensure that our input sequences all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3dd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a list of lists (in other words, a list of dense vectors whose values correspond to respective tokens' frequency rankings)\n",
    "# We will reasonably shorten these vectors to have [max_len] elements\n",
    "# If a vector is shorter than [max_len], it is safe to append 0's to the end of the vector\n",
    "# Otherwise, we truncate it to the first [max_len] entries\n",
    "# Returns a PyTorch tensor, still a 2D matrix, or a list of vectors, but now with the vectors sharing a consistent length\n",
    "def pad(sequences, max_len):\n",
    "    return torch.tensor([seq[:max_len] + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences])\n",
    "\n",
    "# Store padded vectors\n",
    "max_len = 500\n",
    "train_padded = pad(train_indices, max_len)\n",
    "test_padded = pad(test_indices, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NewsDataset(train_padded, train_labels)\n",
    "test_dataset = NewsDataset(test_padded, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create a class for our 1D CNN model with an embedding layer\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc = nn.Linear(128 * (max_len // 2), num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)  # (batch_size, embed_dim, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(vocab) + 1  # Add 1 for padding index\n",
    "embed_dim = 100\n",
    "model = CNNClassifier(vocab_size, embed_dim, num_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

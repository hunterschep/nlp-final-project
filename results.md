# ğŸ† Results

This document captures **all** experimental outcomes in one place.  
Every modelâ€‘variant gets its own subsection containing:

* **Perâ€‘publication scores** â€“ 1,000 heldâ€‘out articles for **each** of the ten sources.  
  `Publication | Accuracy | Precision | Recall |Â F1`
* **Topâ€‘line scores** â€“ aggregate metrics on the full 10,000â€‘article test set.

> â„¹ï¸ **How to use this file:**  
> 1. Run your experiment.  
> 2. Paste the metrics into the matching table below (replace the `â€”` dashes).  
> 3. Commit the updatedÂ `results.md` so future readers can compare techniques at a glance.

---

## Dataset at a Glance
| Split | Articles | Perâ€‘Class Balance |
|-------|----------|-------------------|
| Train | 90,000   | 9,000 each        |
| Test  | 10,000   | 1,000 each        |

<sub>Totals: **100,000** articles, 10 labels, perfectly stratified.</sub>

---

## BaselineÂ ğŸ“‰ Â (UniformÂ RandomÂ â€”Â â€œrollÂ aÂ d10â€)

| Metric | Value |
|--------|-------|
| FitÂ time (s) | **0.036** |
| ScoreÂ time (s) | **0.220** |
| Accuracy | **0.1018** |
| PrecisionÂ (macro) | **0.1018** |
| RecallÂ (macro) | **0.1018** |
| F1Â (macro) | **0.1018** |

*No perâ€‘class table is necessary â€“ every label is hit ~10Â % of the time by chance.*

---

# 1Â Â Classic MethodsÂ ğŸ§®
## 1.1Â Bagâ€‘ofâ€‘WordsÂ (BOW)

### 1.1.1Â MultinomialÂ NaÃ¯veÂ Bayes

| Publication           | Accuracy | Precision | Recall | F1   |
|-----------------------|----------|-----------|--------|------|
| BuzzFeedÂ News         | 0.246    | 0.41      | 0.25   | 0.31 |
| FoxÂ News              | 0.703    | 0.71      | 0.70   | 0.71 |
| CNN                   | 0.244    | 0.48      | 0.24   | 0.32 |
| Reuters               | 0.768    | 0.71      | 0.77   | 0.74 |
| Vice                  | 0.440    | 0.43      | 0.44   | 0.44 |
| TheÂ NewÂ YorkÂ Times    | 0.408    | 0.67      | 0.41   | 0.51 |
| Politico              | 0.505    | 0.45      | 0.51   | 0.48 |
| TheÂ Hill              | 0.613    | 0.63      | 0.61   | 0.62 |
| TheÂ Economist         | 0.723    | 0.87      | 0.72   | 0.79 |
| People                | 0.853    | 0.38      | 0.85   | 0.52 |
| **Topâ€‘Line**          | 0.550    | 0.57      | 0.55   | 0.54 |


### 1.1.2Â LinearÂ SVM (BOWÂ features)

| Publication           | Accuracy | Precision | Recall | F1   |
|-----------------------|----------|-----------|--------|------|
| BuzzFeedÂ News         | 0.518    | 0.59      | 0.52   | 0.55 |
| FoxÂ News              | 0.788    | 0.92      | 0.79   | 0.85 |
| CNN                   | 0.585    | 0.66      | 0.58   | 0.62 |
| Reuters               | 0.978    | 0.96      | 0.98   | 0.97 |
| Vice                  | 0.783    | 0.67      | 0.78   | 0.72 |
| TheÂ NewÂ YorkÂ Times    | 0.767    | 0.80      | 0.77   | 0.78 |
| Politico              | 0.696    | 0.64      | 0.70   | 0.67 |
| TheÂ Hill              | 0.768    | 0.80      | 0.77   | 0.78 |
| TheÂ Economist         | 0.940    | 0.90      | 0.94   | 0.92 |
| People                | 0.826    | 0.71      | 0.83   | 0.77 |
| **Topâ€‘Line**          | 0.760    | 0.77      | 0.76   | 0.76 |


---

## 1.2Â Word2Vec Averages

### 1.2.1Â LogisticÂ Regression
| Publication | Accuracy | Precision | Recall | F1 |
|-------------|----------|-----------|--------|----|
| BuzzFeedÂ News | â€” | â€” | â€” | â€” |
| FoxÂ News | â€” | â€” | â€” | â€” |
| CNN | â€” | â€” | â€” | â€” |
| Reuters | â€” | â€” | â€” | â€” |
| Vice | â€” | â€” | â€” | â€” |
| TheÂ NewÂ YorkÂ Times | â€” | â€” | â€” | â€” |
| Politico | â€” | â€” | â€” | â€” |
| TheÂ Hill | â€” | â€” | â€” | â€” |
| TheÂ Economist | â€” | â€” | â€” | â€” |
| People | â€” | â€” | â€” | â€” |
| **Topâ€‘Line** | â€” | â€” | â€” | â€” |

### 1.2.2Â SVM (RBFÂ KernelÂ +Â W2V)

| Publication           | Accuracy | Precision | Recall | F1   |
|-----------------------|----------|-----------|--------|------|
| BuzzFeedÂ News         | 0.498    | 0.63      | 0.50   | 0.56 |
| FoxÂ News              | 0.728    | 0.76      | 0.73   | 0.74 |
| CNN                   | 0.725    | 0.67      | 0.72   | 0.69 |
| Reuters               | 0.911    | 0.90      | 0.91   | 0.90 |
| Vice                  | 0.813    | 0.78      | 0.81   | 0.80 |
| TheÂ NewÂ YorkÂ Times    | 0.723    | 0.79      | 0.72   | 0.76 |
| Politico              | 0.705    | 0.65      | 0.70   | 0.68 |
| TheÂ Hill              | 0.784    | 0.87      | 0.78   | 0.82 |
| TheÂ Economist         | 0.950    | 0.88      | 0.95   | 0.91 |
| People                | 0.914    | 0.81      | 0.91   | 0.86 |
| **Topâ€‘Line**          | 0.780    | 0.77      | 0.78   | 0.77 |


---

#Â 2Â Â NeuralÂ MethodsÂ ğŸ§ 
## 2.1Â 1â€‘D ConvolutionalÂ NeuralÂ NetworkÂ (CNNâ€‘Text)
| Publication | Accuracy | Precision | Recall | F1 |
|-------------|----------|-----------|--------|----|
| BuzzFeedÂ News | â€” | â€” | â€” | â€” |
| FoxÂ News | â€” | â€” | â€” | â€” |
| CNN | â€” | â€” | â€” | â€” |
| Reuters | â€” | â€” | â€” | â€” |
| Vice | â€” | â€” | â€” | â€” |
| TheÂ NewÂ YorkÂ Times | â€” | â€” | â€” | â€” |
| Politico | â€” | â€” | â€” | â€” |
| TheÂ Hill | â€” | â€” | â€” | â€” |
| TheÂ Economist | â€” | â€” | â€” | â€” |
| People | â€” | â€” | â€” | â€” |
| **Topâ€‘Line** | â€” | â€” | â€” | â€” |

## 2.2Â Feedâ€‘Forward NeuralÂ NetworkÂ (FFNN)
| Publication | Accuracy | Precision | Recall | F1 |
|-------------|----------|-----------|--------|----|
| BuzzFeedÂ News | â€” | 0.50 | 0.55 | 0.53 |
| FoxÂ News | â€” | 0.63 | 0.54 | 0.58 |
| CNN | â€” | 0.64 | 0.48 | 0.55 |
| Reuters | â€” | 0.79 | 0.89 | 0.84 |
| Vice | â€” | 0.83 | 0.75 | 0.79 |
| TheÂ NewÂ YorkÂ Times | â€” | 0.69 | 0.70 | 0.70 |
| Politico | â€” | 0.61 | 0.68 | 0.64 |
| TheÂ Hill | â€” | 0.84 | 0.79 | 0.81 |
| TheÂ Economist | â€” | 0.90 | 0.95 | 0.93 |
| People | â€” | 0.74 | 0.84 | 0.79 |
| **Topâ€‘Line** | 0.72 | 0.72 | 0.72 | 0.71 |

---

#Â 3Â Â Ruleâ€‘BasedÂ ApproachÂ âœï¸ (Editorial StyleÂ GuideÂ Rules)
| Publication | Accuracy | Precision | Recall | F1 |
|-------------|----------|-----------|--------|----|
| BuzzFeedÂ News | â€” | â€” | â€” | â€” |
| FoxÂ News | â€” | â€” | â€” | â€” |
| CNN | â€” | â€” | â€” | â€” |
| Reuters | â€” | â€” | â€” | â€” |
| Vice | â€” | â€” | â€” | â€” |
| TheÂ NewÂ YorkÂ Times | â€” | â€” | â€” | â€” |
| Politico | â€” | â€” | â€” | â€” |
| TheÂ Hill | â€” | â€” | â€” | â€” |
| TheÂ Economist | â€” | â€” | â€” | â€” |
| People | â€” | â€” | â€” | â€” |
| **Topâ€‘Line** | â€” | â€” | â€” | â€” |

---

#Â 4Â Â LargeÂ LanguageÂ ModelÂ (LLM)Â âš¡
## 4.1Â Mistralâ€‘7BÂ (0â€‘shot, system prompt = *â€œPredict publisherâ€*)
| Publication | Accuracy | Precision | Recall | F1 |
|-------------|----------|-----------|--------|----|
| BuzzFeedÂ News | â€” | â€” | â€” | â€” |
| FoxÂ News | â€” | â€” | â€” | â€” |
| CNN | â€” | â€” | â€” | â€” |
| Reuters | â€” | â€” | â€” | â€” |
| Vice | â€” | â€” | â€” | â€” |
| TheÂ NewÂ YorkÂ Times | â€” | â€” | â€” | â€” |
| Politico | â€” | â€” | â€” | â€” |
| TheÂ Hill | â€” | â€” | â€” | â€” |
| TheÂ Economist | â€” | â€” | â€” | â€” |
| People | â€” | â€” | â€” | â€” |
| **Topâ€‘Line** | â€” | â€” | â€” | â€” |

---

#Â 5Â Â Transformerâ€‘BasedÂ EncodersÂ ğŸ¦¾
## 5.1Â BERTÂ (Base, uncased)Â +Â [CLS]Â Logistic Head
| Publication | Accuracy | Precision | Recall | F1 |
|-------------|----------|-----------|--------|----|
| BuzzFeedÂ News | â€” | â€” | â€” | â€” |
| FoxÂ News | â€” | â€” | â€” | â€” |
| CNN | â€” | â€” | â€” | â€” |
| Reuters | â€” | â€” | â€” | â€” |
| Vice | â€” | â€” | â€” | â€” |
| TheÂ NewÂ YorkÂ Times | â€” | â€” | â€” | â€” |
| Politico | â€” | â€” | â€” | â€” |
| TheÂ Hill | â€” | â€” | â€” | â€” |
| TheÂ Economist | â€” | â€” | â€” | â€” |
| People | â€” | â€” | â€” | â€” |
| **Topâ€‘Line** | â€” | â€” | â€” | â€” |

---

## ğŸ“ŒÂ NextÂ Steps
1. **Run** an experiment.  
2. **Replace** the `â€”` placeholders with real numbers (keep three decimals).  
3. **Add** notes under the table if hyperâ€‘parameters or tricks are critical.  